{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8dcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "path = 'C:/Users/Asus/AssignmentTest/text_files_fresh'\n",
    "# Create positional index\n",
    "positional_index = {}\n",
    "for doc_index in range(1, 1000):  # Include the last file\n",
    "    with open(os.path.join(path, f'file{doc_index}.txt'), 'r') as input_file:\n",
    "        text = input_file.read()\n",
    "#         preprocessed_text = preprocess(text)\n",
    "#         tokens = word_tokenize(preprocessed_text)\n",
    "        tokens = word_tokenize(text)\n",
    "        for position, word in enumerate(tokens):\n",
    "            #if word alpha\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {}\n",
    "            if doc_index not in positional_index[word]:\n",
    "                positional_index[word][doc_index] = []\n",
    "            positional_index[word][doc_index].append(position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db63479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the positional index\n",
    "positional_index = dict(sorted(positional_index.items()))\n",
    "\n",
    "# Save the positional index to a pickle file\n",
    "output_filepath = 'positional_index.pkl'  # Specify the output file path\n",
    "with open(output_filepath, \"wb\") as f:\n",
    "    pickle.dump(positional_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6425ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{19: [62, 69], 22: [86], 36: [13], 47: [27], 55: [50, 57], 103: [19], 133: [46], 157: [65], 173: [7], 222: [19, 47, 51, 67, 71], 223: [3, 16, 29], 302: [45], 305: [38], 306: [12, 17], 367: [11, 13, 22], 385: [12], 388: [16], 414: [25, 49], 434: [30], 510: [11], 512: [25, 35], 521: [13, 60, 73, 85], 569: [2, 48, 57], 597: [13], 605: [22], 631: [10, 23], 687: [21], 703: [52], 718: [8, 31, 72], 760: [2, 41, 54], 765: [37], 778: [27], 786: [16], 828: [15], 860: [10], 865: [22, 26], 878: [32, 44], 904: [11, 19, 23], 941: [21], 945: [56], 956: [6], 988: [2]}\n"
     ]
    }
   ],
   "source": [
    "print(positional_index['power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f44650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Queryprocess(query_tokens, positional_index):\n",
    "    results = set()\n",
    "    for doc_id in positional_index.get(query_tokens[0], {}):  # Start with documents containing the 1st token\n",
    "        positions = positional_index[query_tokens[0]].get(doc_id, [])\n",
    "        for pos in positions:  # Iterate through all positions of the 1st token\n",
    "            match = True\n",
    "            for i in range(1, len(query_tokens)):\n",
    "                next_token = query_tokens[i]\n",
    "                next_positions = positional_index.get(next_token, {}).get(doc_id, [])\n",
    "                if not next_positions:  # Word not found in document\n",
    "                    match = False\n",
    "                    break\n",
    "                if pos + i not in next_positions:  # Word not found in the correct position\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:  # Full phrase match found\n",
    "                results.add(doc_id)\n",
    "                break\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9900c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess(text):\n",
    "    text_lower = text.lower()\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    tokens_without_stopwords = [token for token in tokens if token not in stopwords_]\n",
    "    text_without_stopwords = ' '.join(tokens_without_stopwords)\n",
    "    text_without_punctuation = re.sub(r'[^\\w\\s]', '', text_without_stopwords)\n",
    "    return text_without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faab43eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of queries:1\n",
      "Query 1 : power supply\n",
      "Number of documents retrieved for query 1 : 13\n",
      "Names of documents retrieved for query 1 : [512, 521, 988, 367, 305, 434, 19, 597, 55, 760, 828, 222, 223]\n"
     ]
    }
   ],
   "source": [
    "# Number of queries\n",
    "N = int(input(\"Enter number of queries:\"))\n",
    "\n",
    "# Process each query\n",
    "for i in range(N):\n",
    "    print(\"Query\", i + 1, \":\", end=\" \")\n",
    "    query = input()\n",
    "    query_tokens = word_tokenize(query)\n",
    "    results = Queryprocess(query_tokens, positional_index)\n",
    "    print(\"Number of documents retrieved for query\", i + 1, \":\", len(results))\n",
    "    print(\"Names of documents retrieved for query\", i + 1, \":\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791a8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
