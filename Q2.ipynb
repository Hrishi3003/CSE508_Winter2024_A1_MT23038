{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7f85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "re.compile('(.*)')\n",
    "preProcessLemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ee3670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file10.txt\n",
      "file100.txt\n",
      "file101.txt\n",
      "file102.txt\n"
     ]
    }
   ],
   "source": [
    "path='C:/Users/Asus/AssignmentTest/text_files_fresh'\n",
    "list1=os.listdir(path)\n",
    "for i in range(1,5):\n",
    "    print(list1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e403f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loving: ['file1.txt', 'file254.txt', 'file391.txt', 'file723.txt']\n",
      "\n",
      "vintage: ['file1.txt', 'file150.txt', 'file197.txt', 'file278.txt', 'file422.txt', 'file439.txt', 'file494.txt', 'file51.txt', 'file597.txt', 'file638.txt', 'file674.txt', 'file725.txt', 'file737.txt', 'file827.txt', 'file847.txt', 'file895.txt', 'file907.txt', 'file936.txt']\n",
      "\n",
      "springs: ['file1.txt', 'file272.txt', 'file469.txt', 'file806.txt', 'file937.txt']\n",
      "\n",
      "strat: ['file1.txt', 'file149.txt', 'file163.txt', 'file197.txt', 'file241.txt', 'file245.txt', 'file25.txt', 'file253.txt', 'file345.txt', 'file353.txt', 'file380.txt', 'file396.txt', 'file400.txt', 'file422.txt', 'file440.txt', 'file455.txt', 'file457.txt', 'file469.txt', 'file519.txt', 'file529.txt', 'file559.txt', 'file565.txt', 'file579.txt', 'file611.txt', 'file626.txt', 'file650.txt', 'file652.txt', 'file691.txt', 'file725.txt', 'file801.txt', 'file838.txt', 'file853.txt', 'file90.txt', 'file940.txt', 'file978.txt', 'file993.txt']\n",
      "\n",
      "good: ['file1.txt', 'file103.txt', 'file106.txt', 'file110.txt', 'file111.txt', 'file115.txt', 'file118.txt', 'file13.txt', 'file137.txt', 'file141.txt', 'file143.txt', 'file154.txt', 'file155.txt', 'file157.txt', 'file159.txt', 'file16.txt', 'file160.txt', 'file162.txt', 'file163.txt', 'file164.txt', 'file166.txt', 'file172.txt', 'file174.txt', 'file175.txt', 'file176.txt', 'file179.txt', 'file18.txt', 'file189.txt', 'file19.txt', 'file2.txt', 'file204.txt', 'file207.txt', 'file210.txt', 'file217.txt', 'file220.txt', 'file234.txt', 'file235.txt', 'file240.txt', 'file245.txt', 'file252.txt', 'file254.txt', 'file265.txt', 'file274.txt', 'file277.txt', 'file28.txt', 'file282.txt', 'file288.txt', 'file29.txt', 'file292.txt', 'file293.txt', 'file299.txt', 'file30.txt', 'file304.txt', 'file305.txt', 'file311.txt', 'file316.txt', 'file321.txt', 'file325.txt', 'file332.txt', 'file338.txt', 'file342.txt', 'file347.txt', 'file354.txt', 'file355.txt', 'file358.txt', 'file362.txt', 'file367.txt', 'file37.txt', 'file378.txt', 'file390.txt', 'file393.txt', 'file396.txt', 'file4.txt', 'file40.txt', 'file400.txt', 'file404.txt', 'file407.txt', 'file413.txt', 'file422.txt', 'file43.txt', 'file44.txt', 'file455.txt', 'file46.txt', 'file471.txt', 'file493.txt', 'file497.txt', 'file499.txt', 'file501.txt', 'file503.txt', 'file513.txt', 'file522.txt', 'file525.txt', 'file526.txt', 'file532.txt', 'file536.txt', 'file539.txt', 'file541.txt', 'file544.txt', 'file549.txt', 'file551.txt', 'file571.txt', 'file575.txt', 'file58.txt', 'file584.txt', 'file585.txt', 'file586.txt', 'file595.txt', 'file597.txt', 'file598.txt', 'file603.txt', 'file610.txt', 'file619.txt', 'file624.txt', 'file625.txt', 'file626.txt', 'file628.txt', 'file634.txt', 'file637.txt', 'file645.txt', 'file65.txt', 'file655.txt', 'file656.txt', 'file660.txt', 'file674.txt', 'file676.txt', 'file678.txt', 'file680.txt', 'file681.txt', 'file689.txt', 'file703.txt', 'file704.txt', 'file711.txt', 'file713.txt', 'file715.txt', 'file716.txt', 'file72.txt', 'file720.txt', 'file722.txt', 'file731.txt', 'file736.txt', 'file739.txt', 'file748.txt', 'file755.txt', 'file765.txt', 'file769.txt', 'file77.txt', 'file770.txt', 'file772.txt', 'file775.txt', 'file777.txt', 'file78.txt', 'file782.txt', 'file786.txt', 'file793.txt', 'file794.txt', 'file795.txt', 'file8.txt', 'file800.txt', 'file807.txt', 'file813.txt', 'file815.txt', 'file816.txt', 'file819.txt', 'file821.txt', 'file827.txt', 'file841.txt', 'file843.txt', 'file844.txt', 'file85.txt', 'file850.txt', 'file857.txt', 'file859.txt', 'file860.txt', 'file864.txt', 'file867.txt', 'file870.txt', 'file883.txt', 'file885.txt', 'file888.txt', 'file890.txt', 'file896.txt', 'file9.txt', 'file90.txt', 'file907.txt', 'file908.txt', 'file911.txt', 'file912.txt', 'file913.txt', 'file917.txt', 'file918.txt', 'file919.txt', 'file924.txt', 'file925.txt', 'file931.txt', 'file938.txt', 'file941.txt', 'file942.txt', 'file944.txt', 'file96.txt', 'file974.txt', 'file976.txt', 'file983.txt', 'file984.txt', 'file991.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initalizing empty dictonary to store invert index\n",
    "index = {}\n",
    "# Process each file in the directory\n",
    "for filename in list1:\n",
    "    with open(os.path.join(path, filename), \"r\") as file:\n",
    "        # Read file contents\n",
    "        fileContent = file.read()\n",
    "        #Tokenize each word from the open file\n",
    "        Tkens = word_tokenize(fileContent)\n",
    "\n",
    "         # add postings to the dictionary index\n",
    "        for token in Tkens:\n",
    "            if token not in index: #token not present then initialize new \n",
    "                index[token] = [] \n",
    "            if filename not in index[token]:\n",
    "                index[token].append(filename)\n",
    "# Iterate over the keys of the index dictionary, limiting to 5 items\n",
    "for i, token in enumerate(index.keys()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    # Print the token and its associated postings list\n",
    "    print(f\"{token}: {index[token]}\")\n",
    "    print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c988e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save postings to file\n",
    "with open('index.pkl', 'wb') as f:\n",
    "    pickle.dump(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27bc5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: file1.txt\n",
      "\n",
      "1: file10.txt\n",
      "\n",
      "2: file100.txt\n",
      "\n",
      "3: file101.txt\n",
      "\n",
      "4: file102.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_IdMap={}\n",
    "i=0\n",
    "doc_names = \"\"\n",
    "for filename in os.listdir(path):\n",
    "    f = os.path.join(path, filename)\n",
    "    if os.path.isfile(f):\n",
    "        file_IdMap[i]=filename\n",
    "        i += 1\n",
    "for i, token in enumerate(file_IdMap.keys()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    # Print the token and its associated postings list\n",
    "    print(f\"{token}: {file_IdMap[token]}\")\n",
    "    print('')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283e3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fileMapping', 'wb') as fh:\n",
    "   pickle.dump(file_IdMap, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a82e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text):\n",
    "    # lowercase\n",
    "    text_lower = input_text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    text_tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    #exclusion of stopwords from the string and removal of puncations\n",
    "    swords = stopwords.words('english')\n",
    "    tokenstopwords = [token for token in text_tokens if token not in swords]\n",
    "    stopwordsRemoved = ' '.join(tokenstopwords)\n",
    "    text_no_punc = re.sub(r'[^\\w\\s]', '', stopwordsRemoved)\n",
    "    \n",
    "    #removing extra spaces \n",
    "    text_cleaned = text_no_punc.replace(\"  \", \" \")\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b278397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(set1, set2, operation):\n",
    "    set_result = set()\n",
    "    universal_set = set(file_IdMap.values())\n",
    "    if operation == 'AND':\n",
    "        set_result = set1.intersection(set2)\n",
    "      \n",
    "    elif operation == 'AND NOT':\n",
    "        set_result = set1.difference(set2)\n",
    "       \n",
    "    elif operation == 'OR':\n",
    "        set_result = set1.union(set2)\n",
    "       \n",
    "    elif operation == 'OR NOT':\n",
    "        set_result = set1.union(set1, universal_set - set2)\n",
    "       \n",
    "\n",
    "    return set_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5355aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryProcess(query, operators, index):\n",
    "    fileNames = []\n",
    "\n",
    "    for i, op in enumerate(operators):\n",
    "        op = op.strip()\n",
    "        if i == 0:\n",
    "            word1= index[query[i]] if query[i] in index else []\n",
    "            word2= index[query[i+1]] if query[i+1] in index else []\n",
    "\n",
    "\n",
    "            if op == \"AND\":\n",
    "                fileNames= merge_lists(set(word1.copy()), set(word2.copy()), 'AND')\n",
    "            elif op == \"OR\":\n",
    "                fileNames= merge_lists(set(word1.copy()), set(word2.copy().copy()), 'OR')\n",
    "            elif op == \"AND NOT\":\n",
    "                fileNames= merge_lists(set(word1.copy()), set(word2.copy().copy()), 'AND NOT')\n",
    "            elif op == \"OR NOT\":\n",
    "                fileNames= merge_lists(set(word1.copy()), set(word2.copy().copy()), 'OR NOT')\n",
    "            else:\n",
    "                print(\"Invalid Query!\")\n",
    "                return None\n",
    "        else:\n",
    "            if query[i + 1] in index:\n",
    "                word2 = index[query[i + 1]]\n",
    "            else:\n",
    "                word2 = []\n",
    "            if op == \"AND\":\n",
    "                fileNames= merge_lists(set(fileNames.copy()), set(word2.copy()), 'AND')\n",
    "            elif op == \"OR\":\n",
    "                fileNames= merge_lists(set(fileNames.copy()), set(word2.copy()), 'OR')\n",
    "            elif op == \"AND NOT\":\n",
    "                fileNames= merge_lists(set(fileNames.copy()), set(word2.copy()), 'AND NOT')\n",
    "            elif op == \"OR NOT\":\n",
    "                fileNames = merge_lists(set(fileNames.copy()), set(word2.copy()), 'OR NOT')\n",
    "            else:\n",
    "                print(\"Invalid Query\")\n",
    "                return None\n",
    "        \n",
    "\n",
    "    return fileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7440c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries: 1\n",
      "Query  1 :\n",
      "Enter input sequence: power supply\n",
      "Enter operation sequence: AND\n",
      "['power', 'supply']\n",
      "['AND']\n",
      "Query  1 : power AND supply\n",
      "Number of documents retrieved for query  1 :  14\n",
      "Names of the documents retrieved for query  1 :  {'file367.txt', 'file434.txt', 'file512.txt', 'file222.txt', 'file521.txt', 'file597.txt', 'file687.txt', 'file760.txt', 'file988.txt', 'file19.txt', 'file305.txt', 'file828.txt', 'file55.txt', 'file223.txt'}\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries: \"))  # number of queries\n",
    "queries = []\n",
    "operator_list = []\n",
    "\n",
    "# Get queries and operators from the user\n",
    "for p in range(N):\n",
    "    print(\"Query \", p + 1, \":\")\n",
    "    x = input(\"Enter input sequence: \")\n",
    "    queries.append(x)\n",
    "    x = input(\"Enter operation sequence: \")\n",
    "    operator_list.append(x)\n",
    "\n",
    "# Process each query\n",
    "for k in range(N):  # for each query\n",
    "    query = preprocess(queries[k])\n",
    "    operators = operator_list[k]\n",
    "    operators = operators.upper().split(\",\")  # get operators\n",
    "    query = word_tokenize(query)\n",
    "    print(query)\n",
    "    print(operators)\n",
    "\n",
    "    print(\"Query \",k+1,\":\",end=\" \")\n",
    "\n",
    "    if(len(query) != (len(operators)+1)):\n",
    "        print(\"Invalid query\")\n",
    "    else:\n",
    "        for i in range(len(query)-1):\n",
    "            print(query[i]+\" \"+operators[i]+\" \",end=\"\")\n",
    "        print(query[len(query)-1])\n",
    "        fileNames =queryProcess(query, operators,index)\n",
    "        print(\"Number of documents retrieved for query \",k+1,\": \", len(fileNames))\n",
    "\n",
    "        print(\"Names of the documents retrieved for query \", k + 1, \": \", fileNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef00dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
